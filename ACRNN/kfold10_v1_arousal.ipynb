{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca4644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import ACRNN\n",
    "from eegdataset import *\n",
    "from prepare_data_ACRNN import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import os\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27fcd9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def log2txt(filename, content):\n",
    "    print(content)\n",
    "    os.makedirs('Result', exist_ok=True)\n",
    "    log_path = os.path.join(os.getcwd(), 'Result', filename)\n",
    "    with open(log_path, 'a') as file:\n",
    "        file.write(str(content) + '\\n')\n",
    "\n",
    "\n",
    "def log_setting(log_filename, args:dict):\n",
    "    log2txt(log_filename, f\"Experiment started at {datetime.now()}\")\n",
    "    log2txt(log_filename, f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    log2txt(log_filename, f\"KFolds: {args['kfolds']}, Random state: {args['random_state']}\")\n",
    "    log2txt(log_filename, f\"Epochs: {args['epochs']}, Batch size: {args['batch_size']}, Learning rate: {args['lr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26df0791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_experiment(\n",
    "    original_data_path,\n",
    "    processed_data_dir,\n",
    "    subject_ids_to_process,\n",
    "    label_type,\n",
    "    kfolds=10,\n",
    "    epochs=500,\n",
    "    batch_size=64,\n",
    "    lr=0.001,\n",
    "    validation_split_ratio=0.2,\n",
    "    random_state=42,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    log_filename='result.txt',\n",
    "    patience=10\n",
    "):\n",
    "    log_setting(log_filename, {\n",
    "        'kfolds': kfolds, 'random_state': random_state,\n",
    "        'epochs': epochs, 'batch_size': batch_size, 'lr': lr,\n",
    "    })\n",
    "\n",
    "    seed_all(random_state)\n",
    "    \n",
    "    all_subjects_mean_accuracies = []\n",
    "\n",
    "    for sub_idx in subject_ids_to_process:\n",
    "        log2txt(log_filename, f\"\\n============================== Processing Subject s{sub_idx+1:02d} ==============================\")\n",
    "\n",
    "        dataset = EEGDataset(subject_ids=[sub_idx])\n",
    "        kf = KFold(n_splits=kfolds, shuffle=True, random_state=random_state)\n",
    "\n",
    "        fold_acc = []\n",
    "\n",
    "        for fold, (train_indices, test_indices) in enumerate(kf.split(range(len(dataset)))):\n",
    "            log2txt(log_filename, f\"Fold: {fold+1:2d}/{kfolds}:\")\n",
    "            # ============================================ Dataset ============================================\n",
    "            train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "            test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "            model = ACRNN().to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "            # ============================================ Train ============================================\n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                correct, total = 0, 0\n",
    "                running_loss = 0.0\n",
    "                for batch_X, batch_y in train_loader:\n",
    "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += batch_y.size(0)\n",
    "                    correct += (predicted == batch_y).sum().cpu().item()\n",
    "\n",
    "                train_acc = correct / total\n",
    "                if (epoch + 1) % (epochs // 10) == 0 or epoch == 0 or epoch == epochs - 1:\n",
    "                    log2txt(log_filename, f\"Epoch {epoch+1:4d}/{epochs} | Average Train Loss: {running_loss / len(train_loader):.6f}\"\n",
    "                          f\" | Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "            # ============================================ Test ============================================\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                correct_test, total_test = 0, 0\n",
    "                test_loss = 0.0\n",
    "                for batch_X, batch_y in test_loader:\n",
    "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                    outputs = model(batch_X)\n",
    "\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total_test += batch_y.size(0)\n",
    "                    correct_test += (predicted == batch_y).sum().cpu().item()\n",
    "\n",
    "                test_acc = correct_test / total_test\n",
    "                log2txt(log_filename, f\"Fold: {fold+1:2d} | Average Test Loss: {test_loss / len(test_loader):.6f}\"\n",
    "                        f\" | Final Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "            fold_acc.append(test_acc)\n",
    "        \n",
    "        \n",
    "        mean_acc = np.mean(fold_acc)\n",
    "        log2txt(log_filename, f\"\\nSubject s{sub_idx+1:02d}: Mean Accuracy: {mean_acc:.4f} | std: {np.std(fold_acc):.4f}\")\n",
    "        all_subjects_mean_accuracies.append(mean_acc)\n",
    "        \n",
    "    \n",
    "    all_subjects_std_accuracies = np.std(all_subjects_mean_accuracies)\n",
    "    log2txt(log_filename, f\"\\nAll Subjects Mean Accuracy: {np.mean(all_subjects_mean_accuracies):.4f} Â± {all_subjects_std_accuracies:.4f}\")\n",
    "    return all_subjects_mean_accuracies, all_subjects_std_accuracies\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2337b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on cuda\n",
      "Experiment started at 2025-06-21 10:56:29.908205\n",
      "Device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "KFolds: 10, Random state: 42\n",
      "Epochs: 300, Batch size: 10, Learning rate: 0.0001\n",
      "\n",
      "============================== Processing Subject s12 ==============================\n",
      "data shape: (800, 32, 384), labels shape: (800,)\n",
      "Fold:  1/10:\n",
      "Epoch    1/300 | Average Train Loss: 0.631965 | Train Acc: 0.8194\n",
      "Epoch   30/300 | Average Train Loss: 0.478960 | Train Acc: 0.8194\n",
      "Epoch   60/300 | Average Train Loss: 0.401584 | Train Acc: 0.8194\n",
      "Epoch   90/300 | Average Train Loss: 0.326511 | Train Acc: 0.8194\n",
      "Epoch  120/300 | Average Train Loss: 0.247944 | Train Acc: 0.8194\n",
      "Epoch  150/300 | Average Train Loss: 0.207486 | Train Acc: 0.8194\n",
      "Epoch  180/300 | Average Train Loss: 0.154323 | Train Acc: 0.9750\n",
      "Epoch  210/300 | Average Train Loss: 0.132764 | Train Acc: 0.9889\n",
      "Epoch  240/300 | Average Train Loss: 0.117872 | Train Acc: 0.9903\n",
      "Epoch  270/300 | Average Train Loss: 0.092801 | Train Acc: 0.9875\n",
      "Epoch  300/300 | Average Train Loss: 0.072835 | Train Acc: 0.9903\n",
      "Fold:  1 | Average Test Loss: 0.046393 | Final Test Acc: 1.0000\n",
      "Fold:  2/10:\n",
      "Epoch    1/300 | Average Train Loss: 0.668233 | Train Acc: 0.8181\n",
      "Epoch   30/300 | Average Train Loss: 0.479010 | Train Acc: 0.8181\n",
      "Epoch   60/300 | Average Train Loss: 0.395545 | Train Acc: 0.8181\n",
      "Epoch   90/300 | Average Train Loss: 0.328267 | Train Acc: 0.8181\n",
      "Epoch  120/300 | Average Train Loss: 0.276436 | Train Acc: 0.8181\n",
      "Epoch  150/300 | Average Train Loss: 0.225938 | Train Acc: 0.8181\n",
      "Epoch  180/300 | Average Train Loss: 0.165822 | Train Acc: 0.9542\n",
      "Epoch  210/300 | Average Train Loss: 0.137814 | Train Acc: 0.9806\n",
      "Epoch  240/300 | Average Train Loss: 0.110716 | Train Acc: 0.9875\n",
      "Epoch  270/300 | Average Train Loss: 0.099811 | Train Acc: 0.9903\n",
      "Epoch  300/300 | Average Train Loss: 0.068304 | Train Acc: 0.9972\n",
      "Fold:  2 | Average Test Loss: 0.050625 | Final Test Acc: 0.9875\n",
      "Fold:  3/10:\n",
      "Epoch    1/300 | Average Train Loss: 0.707968 | Train Acc: 0.1764\n",
      "Epoch   30/300 | Average Train Loss: 0.484566 | Train Acc: 0.8236\n",
      "Epoch   60/300 | Average Train Loss: 0.442395 | Train Acc: 0.8236\n",
      "Epoch   90/300 | Average Train Loss: 0.363609 | Train Acc: 0.8236\n",
      "Epoch  120/300 | Average Train Loss: 0.281341 | Train Acc: 0.8236\n",
      "Epoch  150/300 | Average Train Loss: 0.223827 | Train Acc: 0.8236\n",
      "Epoch  180/300 | Average Train Loss: 0.193472 | Train Acc: 0.9472\n",
      "Epoch  210/300 | Average Train Loss: 0.154820 | Train Acc: 0.9653\n",
      "Epoch  240/300 | Average Train Loss: 0.125332 | Train Acc: 0.9806\n",
      "Epoch  270/300 | Average Train Loss: 0.104049 | Train Acc: 0.9806\n",
      "Epoch  300/300 | Average Train Loss: 0.078057 | Train Acc: 0.9917\n",
      "Fold:  3 | Average Test Loss: 0.073155 | Final Test Acc: 0.9875\n",
      "Fold:  4/10:\n",
      "Epoch    1/300 | Average Train Loss: 0.653410 | Train Acc: 0.8319\n",
      "Epoch   30/300 | Average Train Loss: 0.457248 | Train Acc: 0.8319\n",
      "Epoch   60/300 | Average Train Loss: 0.375119 | Train Acc: 0.8319\n",
      "Epoch   90/300 | Average Train Loss: 0.310286 | Train Acc: 0.8319\n",
      "Epoch  120/300 | Average Train Loss: 0.241391 | Train Acc: 0.8319\n",
      "Epoch  150/300 | Average Train Loss: 0.196327 | Train Acc: 0.8319\n",
      "Epoch  180/300 | Average Train Loss: 0.153281 | Train Acc: 0.9569\n",
      "Epoch  210/300 | Average Train Loss: 0.132431 | Train Acc: 0.9806\n",
      "Epoch  240/300 | Average Train Loss: 0.118614 | Train Acc: 0.9819\n",
      "Epoch  270/300 | Average Train Loss: 0.089434 | Train Acc: 0.9903\n",
      "Epoch  300/300 | Average Train Loss: 0.080790 | Train Acc: 0.9833\n",
      "Fold:  4 | Average Test Loss: 0.735310 | Final Test Acc: 0.9875\n",
      "Fold:  5/10:\n",
      "Epoch    1/300 | Average Train Loss: 0.714150 | Train Acc: 0.1681\n",
      "Epoch   30/300 | Average Train Loss: 0.459106 | Train Acc: 0.8319\n",
      "Epoch   60/300 | Average Train Loss: 0.365903 | Train Acc: 0.8319\n",
      "Epoch   90/300 | Average Train Loss: 0.284774 | Train Acc: 0.8319\n",
      "Epoch  120/300 | Average Train Loss: 0.201953 | Train Acc: 0.8319\n",
      "Epoch  150/300 | Average Train Loss: 0.164672 | Train Acc: 0.9653\n",
      "Epoch  180/300 | Average Train Loss: 0.130896 | Train Acc: 0.9806\n",
      "Epoch  210/300 | Average Train Loss: 0.110092 | Train Acc: 0.9847\n",
      "Epoch  240/300 | Average Train Loss: 0.076047 | Train Acc: 0.9958\n",
      "Epoch  270/300 | Average Train Loss: 0.089090 | Train Acc: 0.9847\n",
      "Epoch  300/300 | Average Train Loss: 0.067441 | Train Acc: 0.9875\n",
      "Fold:  5 | Average Test Loss: 0.163267 | Final Test Acc: 0.9750\n",
      "Fold:  6/10:\n",
      "Epoch    1/300 | Average Train Loss: 0.719972 | Train Acc: 0.1722\n",
      "Epoch   30/300 | Average Train Loss: 0.454230 | Train Acc: 0.8278\n",
      "Epoch   60/300 | Average Train Loss: 0.380546 | Train Acc: 0.8278\n",
      "Epoch   90/300 | Average Train Loss: 0.320561 | Train Acc: 0.8278\n",
      "Epoch  120/300 | Average Train Loss: 0.249779 | Train Acc: 0.8278\n",
      "Epoch  150/300 | Average Train Loss: 0.185678 | Train Acc: 0.8361\n",
      "Epoch  180/300 | Average Train Loss: 0.149969 | Train Acc: 0.9764\n",
      "Epoch  210/300 | Average Train Loss: 0.119281 | Train Acc: 0.9889\n",
      "Epoch  240/300 | Average Train Loss: 0.100962 | Train Acc: 0.9889\n",
      "Epoch  270/300 | Average Train Loss: 0.087578 | Train Acc: 0.9875\n",
      "Epoch  300/300 | Average Train Loss: 0.081236 | Train Acc: 0.9833\n",
      "Fold:  6 | Average Test Loss: 0.285070 | Final Test Acc: 0.9625\n",
      "Fold:  7/10:\n",
      "Epoch    1/300 | Average Train Loss: 0.645595 | Train Acc: 0.8222\n",
      "Epoch   30/300 | Average Train Loss: 0.476144 | Train Acc: 0.8222\n",
      "Epoch   60/300 | Average Train Loss: 0.392981 | Train Acc: 0.8222\n",
      "Epoch   90/300 | Average Train Loss: 0.303666 | Train Acc: 0.8222\n",
      "Epoch  120/300 | Average Train Loss: 0.250333 | Train Acc: 0.8222\n",
      "Epoch  150/300 | Average Train Loss: 0.191477 | Train Acc: 0.8222\n",
      "Epoch  180/300 | Average Train Loss: 0.164287 | Train Acc: 0.9708\n",
      "Epoch  210/300 | Average Train Loss: 0.133851 | Train Acc: 0.9778\n",
      "Epoch  240/300 | Average Train Loss: 0.119396 | Train Acc: 0.9847\n",
      "Epoch  270/300 | Average Train Loss: 0.097029 | Train Acc: 0.9819\n",
      "Epoch  300/300 | Average Train Loss: 0.069413 | Train Acc: 0.9931\n",
      "Fold:  7 | Average Test Loss: 0.130577 | Final Test Acc: 0.9750\n",
      "Fold:  8/10:\n",
      "Epoch    1/300 | Average Train Loss: 0.724818 | Train Acc: 0.1750\n",
      "Epoch   30/300 | Average Train Loss: 0.472902 | Train Acc: 0.8250\n",
      "Epoch   60/300 | Average Train Loss: 0.394605 | Train Acc: 0.8250\n",
      "Epoch   90/300 | Average Train Loss: 0.293242 | Train Acc: 0.8250\n",
      "Epoch  120/300 | Average Train Loss: 0.229830 | Train Acc: 0.8250\n",
      "Epoch  150/300 | Average Train Loss: 0.194261 | Train Acc: 0.9528\n",
      "Epoch  180/300 | Average Train Loss: 0.147336 | Train Acc: 0.9750\n",
      "Epoch  210/300 | Average Train Loss: 0.104016 | Train Acc: 0.9889\n",
      "Epoch  240/300 | Average Train Loss: 0.095421 | Train Acc: 0.9875\n",
      "Epoch  270/300 | Average Train Loss: 0.077366 | Train Acc: 0.9889\n",
      "Epoch  300/300 | Average Train Loss: 0.056121 | Train Acc: 0.9972\n",
      "Fold:  8 | Average Test Loss: 0.245307 | Final Test Acc: 0.9750\n",
      "Fold:  9/10:\n",
      "Epoch    1/300 | Average Train Loss: 0.672182 | Train Acc: 0.8208\n",
      "Epoch   30/300 | Average Train Loss: 0.486794 | Train Acc: 0.8208\n",
      "Epoch   60/300 | Average Train Loss: 0.421508 | Train Acc: 0.8208\n",
      "Epoch   90/300 | Average Train Loss: 0.316669 | Train Acc: 0.8208\n",
      "Epoch  120/300 | Average Train Loss: 0.248848 | Train Acc: 0.8208\n",
      "Epoch  150/300 | Average Train Loss: 0.211516 | Train Acc: 0.8208\n",
      "Epoch  180/300 | Average Train Loss: 0.156459 | Train Acc: 0.9764\n",
      "Epoch  210/300 | Average Train Loss: 0.126675 | Train Acc: 0.9806\n",
      "Epoch  240/300 | Average Train Loss: 0.121826 | Train Acc: 0.9819\n",
      "Epoch  270/300 | Average Train Loss: 0.096692 | Train Acc: 0.9806\n",
      "Epoch  300/300 | Average Train Loss: 0.082163 | Train Acc: 0.9903\n",
      "Fold:  9 | Average Test Loss: 0.100072 | Final Test Acc: 0.9625\n",
      "Fold: 10/10:\n",
      "Epoch    1/300 | Average Train Loss: 0.693269 | Train Acc: 0.4861\n",
      "Epoch   30/300 | Average Train Loss: 0.472708 | Train Acc: 0.8292\n",
      "Epoch   60/300 | Average Train Loss: 0.386390 | Train Acc: 0.8292\n",
      "Epoch   90/300 | Average Train Loss: 0.306012 | Train Acc: 0.8292\n",
      "Epoch  120/300 | Average Train Loss: 0.218036 | Train Acc: 0.8292\n",
      "Epoch  150/300 | Average Train Loss: 0.162455 | Train Acc: 0.9014\n",
      "Epoch  180/300 | Average Train Loss: 0.140884 | Train Acc: 0.9792\n",
      "Epoch  210/300 | Average Train Loss: 0.123170 | Train Acc: 0.9833\n",
      "Epoch  240/300 | Average Train Loss: 0.101139 | Train Acc: 0.9861\n",
      "Epoch  270/300 | Average Train Loss: 0.089150 | Train Acc: 0.9861\n",
      "Epoch  300/300 | Average Train Loss: 0.097344 | Train Acc: 0.9819\n",
      "Fold: 10 | Average Test Loss: 0.258259 | Final Test Acc: 0.9625\n",
      "\n",
      "Subject s12: Mean Accuracy: 0.9775 | std: 0.0122\n",
      "\n",
      "All Subjects Mean Accuracy: 0.9775 Â± 0.0000\n",
      "\n",
      "============================== Final Experiment Results Across All Subjects ==============================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m final_results = run_full_experiment(**EXPERIMENT_PARAMS)\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m============================== Final Experiment Results Across All Subjects ==============================\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m metric, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfinal_results\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m():\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric.replace(\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m).capitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'tuple' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "DEAP_ORIGINAL_DATA_PATH = '../DATA/DEAP/data_preprocessed_python' \n",
    "PROCESSED_DATA_DIR = '../DATA/DEAP/processed_data_for_ACRNN' \n",
    "\n",
    "idx_begin = 13\n",
    "idx_end = 13\n",
    "\n",
    "epochs = 300\n",
    "time_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "EXPERIMENT_PARAMS = {\n",
    "    'original_data_path': DEAP_ORIGINAL_DATA_PATH,\n",
    "    'processed_data_dir': PROCESSED_DATA_DIR,\n",
    "    'subject_ids_to_process': list(range(idx_begin-1, idx_end)), \n",
    "    'label_type': 'A',        # 'A', 'V', 'AV'               \n",
    "    'kfolds': 10,            \n",
    "    'epochs': epochs, \n",
    "    'batch_size': 10,          \n",
    "    'lr': 1e-4,       \n",
    "    'validation_split_ratio': 0.2, # 80% train, 20% validation from training folds\n",
    "    'random_state': 42,        \n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'log_filename': f'ACRNN_DEAP_10folds_Arousal_{time_str}_result.txt',\n",
    "    'patience': epochs // 1\n",
    "}\n",
    "\n",
    "# Check if original data path exists\n",
    "if not os.path.exists(EXPERIMENT_PARAMS['original_data_path']):\n",
    "    print(f\"Error: Original DEAP data path '{EXPERIMENT_PARAMS['original_data_path']}' not found.\")\n",
    "    print(\"Please download the DEAP dataset and place the .dat files in this directory.\")\n",
    "    print(\"Exiting...\")\n",
    "else:\n",
    "    print(f\"Train on {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "    final_results = run_full_experiment(**EXPERIMENT_PARAMS)\n",
    "\n",
    "    print(\"\\n============================== Final Experiment Results Across All Subjects ==============================\")\n",
    "    for metric, value in final_results.items():\n",
    "        print(f\"{metric.replace('_', ' ').capitalize()}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
